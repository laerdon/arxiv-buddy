{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2822cc",
   "metadata": {},
   "source": [
    "# Generating a few JSONs via Claude API\n",
    "\n",
    "Being able to extract a human understandable JSON from a paper can be thought of as a form of encoding. In this notebook, I show that Claude does a good job of generating these JSONs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfb1e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import msglm\n",
    "import base64, httpx\n",
    "import dotenv\n",
    "\n",
    "from msglm import mk_msg\n",
    "from anthropic import Anthropic\n",
    "\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "a_cli = Anthropic()\n",
    "\n",
    "model = models[1] # 'claude-3-7-sonnet-20250219' for cost efficiency\n",
    "assert model == 'claude-sonnet-4-20250514'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anthropic_chat(model: str, msgs: list)->tuple:\n",
    "    \"call the anthropic messages endpoint with `msgs`.\"\n",
    "    r = a_cli.messages.create(model=model, max_tokens=1024, messages=msgs, system=\"You are a helpful assistant that generates descriptive and accurate JSONs.\")\n",
    "    return r, r.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51a76837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_016MfH9iXgJfvQo5raEw3ptp', content=[TextBlock(citations=None, text='```json\\n{\\n  \"Abstract\": \"This paper introduces LLaDA (Large Language Diffusion with mAsking), a diffusion model trained from scratch using pre-training and supervised fine-tuning paradigms. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. The model demonstrates strong scalability, competitive performance with LLaMA3 8B in in-context learning, impressive instruction-following abilities after SFT, and addresses the reversal curse by outperforming GPT-4o in reversal poem completion tasks.\",\\n  \\n  \"Concept_keywords\": [\"diffusion models\", \"masked language modeling\", \"autoregressive alternatives\", \"bidirectional modeling\", \"large language models\", \"probabilistic inference\", \"masked diffusion models\", \"transformer architecture\"],\\n  \\n  \"Methodology\": {\\n    \"keywords\": [\"masked diffusion models\", \"forward process\", \"reverse process\", \"mask predictor\", \"cross-entropy loss\", \"likelihood bound optimization\"],\\n    \"short_summary\": \"LLaDA uses a masked diffusion model approach where a forward process gradually masks tokens independently with probability t, and a reverse process recovers the data distribution by predicting masked tokens. The model is trained using cross-entropy loss computed only on masked tokens, optimizing a likelihood bound for principled generative modeling.\"\\n  },\\n  \\n  \"Experiment_design\": {\\n    \"keywords\": [\"8B parameter model\", \"2.3T training tokens\", \"supervised fine-tuning\", \"benchmark evaluation\", \"scalability analysis\", \"reversal reasoning\"],\\n    \"short_summary\": \"The authors scale LLaDA to 8B parameters, pre-train on 2.3 trillion tokens, and fine-tune on 4.5 million pairs. They evaluate across 15 benchmarks covering general tasks, mathematics, code, and Chinese understanding, comparing against LLaMA models and analyzing scalability up to 10^23 FLOPs.\"\\n  },\\n  \\n  \"Results\": {\\n    \"keywords\": [\"competitive performance\", \"scalability\", \"reversal curse\", \"instruction following\", \"in-context learning\", \"bidirectional reasoning\"],\\n    \"short_summary\": \"LLaDA 8B demonstrates competitive performance with LLaMA3 8B on most benchmarks, surpasses LLaMA2 7B on nearly all tasks, shows strong scalability matching ARM baselines, effectively addresses the reversal curse with consistent forward/reversal performance, and exhibits impressive instruction-following capabilities after SFT.\"\\n  },\\n  \\n  \"Related_work\": {\\n    \"keywords\": [\"diffusion models\", \"discrete diffusion\", \"masked language models\", \"autoregressive alternatives\", \"continuous diffusion\", \"bidirectional modeling\"],\\n    \"how_this_paper_connects\": \"This work builds on masked diffusion models (Austin et al., 2021a; Lou et al., 2023; Ou et al., 2024) and extends them to unprecedented scale (8B parameters) for language modeling. It connects to the broader literature on diffusion models in NLP, discrete diffusion processes, and alternatives to autoregressive modeling, while demonstrating that capabilities traditionally associated with autoregressive models can emerge from diffusion-based approaches.\"\\n  },\\n  \\n  \"Limitations/future_work\": \"The scale of LLaDA remains smaller than leading counterparts, requiring further scaling to fully assess capabilities. The model has yet to undergo reinforcement learning alignment, shows sensitivity to inference hyperparameters, and lacks specialized architectural optimizations. Future work should explore multi-modal capabilities, systematic post-training investigation, and integration with agent-based systems.\",\\n  \\n  \"Novelty\": \"This paper presents the first successful scaling of masked diffusion models to 8B parameters for language modeling, demonstrating that key LLM capabilities (scalability, in-context learning, instruction-following) can emerge from non-autoregressive diffusion models. The work challenges the assumption that these capabilities are inherently tied to autoregressive modeling and establishes diffusion models as a viable alternative to ARMs for large-scale language modeling.\",\\n  \\n  \"Metadata\": {\\n    \"authors\": [\"Shen Nie\", \"Fengqi Zhu\", \"Zebin You\", \"Xiaolu Zhang\", \"Jingyang Ou\", \"Jun Hu\", \"Jun Zhou\", \"Yankai Lin\", \"Ji-Rong Wen\", \"Chongxuan Li\"],\\n    \"link_doi\": \"arXiv:2502.09', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='max_tokens', stop_sequence=None, type='message', usage=In: 66545; Out: 1024; Cache create: 0; Cache read: 0; Total Tokens: 67569; Search: 0)\n"
     ]
    }
   ],
   "source": [
    "json_schema = \"\"\"{\n",
    "Abstract: \n",
    "Concept_keywords: []\n",
    "Methodology: {keywords, short summary}\n",
    "Experiment_design: {keywords, short summary}\n",
    "Results: {keywords, short summary}\n",
    "Related_work: {keywords, *how this paper connects to existing literature }\n",
    "Limitations/future work: {short prose}\n",
    "Novelty: str should be judgement by the LLM, a search for “contribution,” “novel”\n",
    "Metadata: {authors, link/doi, affiliations, publish date, sorted_citations: {title, link}, code availability, data availability }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "llada = \"https://arxiv.org/pdf/2502.09992\"\n",
    "\n",
    "llada_prompt = f\"Please fill out the following json schema with the information from the paper at the end of this message: \\n\\n{json_schema}\"\n",
    "\n",
    "llada_msg = mk_msg(\n",
    "    [llada_prompt, llada], api='anthropic'\n",
    ")\n",
    "\n",
    "r, llada_json = anthropic_chat(msgs=[llada_msg], model=model)\n",
    "\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "394faeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"Abstract\": \"This paper introduces LLaDA (Large Language Diffusion with mAsking), a diffusion model trained from scratch using pre-training and supervised fine-tuning paradigms. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. The model demonstrates strong scalability, competitive performance with LLaMA3 8B in in-context learning, impressive instruction-following abilities after SFT, and addresses the reversal curse by outperforming GPT-4o in reversal poem completion tasks.\",\n",
      "  \n",
      "  \"Concept_keywords\": [\"diffusion models\", \"masked language modeling\", \"autoregressive alternatives\", \"bidirectional modeling\", \"large language models\", \"probabilistic inference\", \"masked diffusion models\", \"transformer architecture\"],\n",
      "  \n",
      "  \"Methodology\": {\n",
      "    \"keywords\": [\"masked diffusion models\", \"forward process\", \"reverse process\", \"mask predictor\", \"cross-entropy loss\", \"likelihood bound optimization\"],\n",
      "    \"short_summary\": \"LLaDA uses a masked diffusion model approach where a forward process gradually masks tokens independently with probability t, and a reverse process recovers the data distribution by predicting masked tokens. The model is trained using cross-entropy loss computed only on masked tokens, optimizing a likelihood bound for principled generative modeling.\"\n",
      "  },\n",
      "  \n",
      "  \"Experiment_design\": {\n",
      "    \"keywords\": [\"8B parameter model\", \"2.3T training tokens\", \"supervised fine-tuning\", \"benchmark evaluation\", \"scalability analysis\", \"reversal reasoning\"],\n",
      "    \"short_summary\": \"The authors scale LLaDA to 8B parameters, pre-train on 2.3 trillion tokens, and fine-tune on 4.5 million pairs. They evaluate across 15 benchmarks covering general tasks, mathematics, code, and Chinese understanding, comparing against LLaMA models and analyzing scalability up to 10^23 FLOPs.\"\n",
      "  },\n",
      "  \n",
      "  \"Results\": {\n",
      "    \"keywords\": [\"competitive performance\", \"scalability\", \"reversal curse\", \"instruction following\", \"in-context learning\", \"bidirectional reasoning\"],\n",
      "    \"short_summary\": \"LLaDA 8B demonstrates competitive performance with LLaMA3 8B on most benchmarks, surpasses LLaMA2 7B on nearly all tasks, shows strong scalability matching ARM baselines, effectively addresses the reversal curse with consistent forward/reversal performance, and exhibits impressive instruction-following capabilities after SFT.\"\n",
      "  },\n",
      "  \n",
      "  \"Related_work\": {\n",
      "    \"keywords\": [\"diffusion models\", \"discrete diffusion\", \"masked language models\", \"autoregressive alternatives\", \"continuous diffusion\", \"bidirectional modeling\"],\n",
      "    \"how_this_paper_connects\": \"This work builds on masked diffusion models (Austin et al., 2021a; Lou et al., 2023; Ou et al., 2024) and extends them to unprecedented scale (8B parameters) for language modeling. It connects to the broader literature on diffusion models in NLP, discrete diffusion processes, and alternatives to autoregressive modeling, while demonstrating that capabilities traditionally associated with autoregressive models can emerge from diffusion-based approaches.\"\n",
      "  },\n",
      "  \n",
      "  \"Limitations/future_work\": \"The scale of LLaDA remains smaller than leading counterparts, requiring further scaling to fully assess capabilities. The model has yet to undergo reinforcement learning alignment, shows sensitivity to inference hyperparameters, and lacks specialized architectural optimizations. Future work should explore multi-modal capabilities, systematic post-training investigation, and integration with agent-based systems.\",\n",
      "  \n",
      "  \"Novelty\": \"This paper presents the first successful scaling of masked diffusion models to 8B parameters for language modeling, demonstrating that key LLM capabilities (scalability, in-context learning, instruction-following) can emerge from non-autoregressive diffusion models. The work challenges the assumption that these capabilities are inherently tied to autoregressive modeling and establishes diffusion models as a viable alternative to ARMs for large-scale language modeling.\",\n",
      "  \n",
      "  \"Metadata\": {\n",
      "    \"authors\": [\"Shen Nie\", \"Fengqi Zhu\", \"Zebin You\", \"Xiaolu Zhang\", \"Jingyang Ou\", \"Jun Hu\", \"Jun Zhou\", \"Yankai Lin\", \"Ji-Rong Wen\", \"Chongxuan Li\"],\n",
      "    \"link_doi\": \"arXiv:2502.09\n"
     ]
    }
   ],
   "source": [
    "print(r.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95999b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uvws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
